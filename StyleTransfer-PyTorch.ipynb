{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "In this notebook we will implement the style transfer technique from [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2015)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).\n",
    "\n",
    "The general idea is to take two images, and produce a new image that reflects the content of one but the artistic \"style\" of the other. We will do this by first formulating a loss function that matches the content and style of each respective image in the feature space of a deep network, and then performing gradient descent on the pixels of the image itself.\n",
    "\n",
    "The deep network we use as a feature extractor is [SqueezeNet](https://arxiv.org/abs/1602.07360), a small model that has been trained on ImageNet. You could use any network, but we chose SqueezeNet here for its small size and efficiency.\n",
    "\n",
    "Here's an example of the images you'll be able to produce by the end of this notebook:\n",
    "\n",
    "![caption](example_styletransfer.png)\n",
    "\n",
    "\n",
    "We will then use this to create a video style transferer and then use it to automatic download videos and styles from the internet and upload them to my [Youtube Channel](https://www.youtube.com/channel/UC2RKwvGB9hrVYQtmPNtCLkw/featured)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory of Python installs: /home/rishirules/anaconda3/lib/python3.7/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "print(\"Directory of Python installs:\",torch.__file__)\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deeplearning.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper functions to deal with images, since we're dealing with real JPEGs, not CIFAR-10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtype = torch.FloatTensor\n",
    "#comment above or below depending if you're on a machine with a GPU set up for PyTorch!\n",
    "dtype = torch.cuda.FloatTensor\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=512):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((size,size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "def preprocess_batch(img, size=512):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((size,size)),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "def deprocess(img):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1.0 / s for s in SQUEEZENET_STD.tolist()]),\n",
    "        T.Normalize(mean=[-m for m in SQUEEZENET_MEAN.tolist()], std=[1, 1, 1]),\n",
    "        T.Lambda(rescale),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    if (img.shape[0])==1:\n",
    "        return transform(img)\n",
    "    else:\n",
    "        return [transform(imgx.reshape(1,3,224,224)) for imgx in img]\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = torch.min(x), torch.max(x)\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def features_from_img(imgpath, imgsize):\n",
    "    img = preprocess(PIL.Image.open(imgpath), size=imgsize)\n",
    "    img_var = Variable(img.type(dtype))\n",
    "     \n",
    "    return extract_features(img_var.to(device), cnn), img_var\n",
    "\n",
    "# Older versions of scipy.misc.imresize yield different results\n",
    "# from newer versions, so we check to make sure scipy is up to date.\n",
    "def check_scipy():\n",
    "    import scipy\n",
    "    vnum = int(scipy.__version__.split('.')[1])\n",
    "   \n",
    "    assert vnum >= 16, \"You must install SciPy >= 0.16.0 to complete this notebook.\"\n",
    "\n",
    "#check_scipy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the dtype to select either the CPU or the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained SqueezeNet model.\n",
    "cnn = torchvision.models.squeezenet1_1(pretrained=True).features.to(device)\n",
    "cnn.type(dtype)\n",
    "\n",
    "\n",
    "\n",
    "# We don't want to train the model any further, so we don't want PyTorch to waste computation \n",
    "# computing gradients on parameters we're never going to update.\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#helper code which takes an image, a model (cnn), and returns a list of\n",
    "# feature maps, one per layer.\n",
    "def extract_features(x, cnn):\n",
    "    \"\"\"\n",
    "    Use the CNN to extract features from the input image x.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Variable of shape (N, C, H, W) holding a minibatch of images that\n",
    "      will be fed to the CNN.\n",
    "    - cnn: A PyTorch model that we will use to extract features.\n",
    "    \n",
    "    Returns:\n",
    "    - features: A list of feature for the input images x extracted using the cnn model.\n",
    "      features[i] is a PyTorch Variable of shape (N, C_i, H_i, W_i); recall that features\n",
    "      from different layers of the network may have different numbers of channels (C_i) and\n",
    "      spatial dimensions (H_i, W_i).\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    cnn = cnn\n",
    "    prev_feat = x\n",
    "    for i, module in enumerate(cnn._modules.values()):\n",
    "        next_feat = module(prev_feat)\n",
    "        features.append(next_feat)\n",
    "        prev_feat = next_feat\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Loss\n",
    "\n",
    "We're going to compute the three components of our loss function now. The loss function is a weighted sum of three terms: content loss + style loss + total variation loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss\n",
    "We can generate an image that reflects the content of one image and the style of another by incorporating both in our loss function. We want to penalize deviations from the content of the content image and deviations from the style of the style image. We can then use this hybrid loss function to perform gradient descent **not on the parameters** of the model, but instead **on the pixel values** of our original image.\n",
    "\n",
    "Let's first write the content loss function. Content loss measures how much the feature map of the generated image differs from the feature map of the source image. We only care about the content representation of one layer of the network (say, layer $\\ell$), that has feature maps $A^\\ell \\in \\mathbb{R}^{1 \\times C_\\ell \\times H_\\ell \\times W_\\ell}$. $C_\\ell$ is the number of filters/channels in layer $\\ell$, $H_\\ell$ and $W_\\ell$ are the height and width. We will work with reshaped versions of these feature maps that combine all spatial positions into one dimension. Let $F^\\ell \\in \\mathbb{R}^{N_\\ell \\times M_\\ell}$ be the feature map for the current image and $P^\\ell \\in \\mathbb{R}^{N_\\ell \\times M_\\ell}$ be the feature map for the content source image where $M_\\ell=H_\\ell\\times W_\\ell$ is the number of elements in each feature map. Each row of $F^\\ell$ or $P^\\ell$ represents the vectorized activations of a particular filter, convolved over all positions of the image. Finally, let $w_c$ be the weight of the content loss term in the loss function.\n",
    "\n",
    "Then the content loss is given by:\n",
    "\n",
    "$L_c = w_c \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content_weight, content_current, content_original):\n",
    "    \"\"\"\n",
    "    Compute the content loss for style transfer.\n",
    "    \n",
    "    Inputs:\n",
    "    - content_weight: Scalar giving the weighting for the content loss.\n",
    "    - content_current: features of the current image; this is a PyTorch Tensor of shape\n",
    "      (1, C_l, H_l, W_l).\n",
    "    - content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l).\n",
    "    \n",
    "    Returns:\n",
    "    - scalar content loss\n",
    "    \"\"\"\n",
    "#     sumy=0\n",
    "    N ,C,H,W=content_current.shape\n",
    "#     for k in range(C):\n",
    "#         for i in range(H):\n",
    "#             for j in range(W):\n",
    "#                 sumy+=(content_current[0][k][i][j]-content_original[0][k][i][j])**2\n",
    "    \n",
    "    sumy=torch.nn.functional.mse_loss(content_current,content_original)\n",
    "    return N*C*H*W*content_weight*sumy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style loss\n",
    "Now we can tackle the style loss. For a given layer $\\ell$, the style loss is defined as follows:\n",
    "\n",
    "First, compute the Gram matrix G which represents the correlations between the responses of each filter, where F is as above. The Gram matrix is an approximation to the covariance matrix -- we want the activation statistics of our generated image to match the activation statistics of our style image, and matching the (approximate) covariance is one way to do that. There are a variety of ways you could do this, but the Gram matrix is nice because it's easy to compute and in practice shows good results.\n",
    "\n",
    "Given a feature map $F^\\ell$ of shape $(1, C_\\ell, M_\\ell)$, the Gram matrix has shape $(1, C_\\ell, C_\\ell)$ and its elements are given by:\n",
    "\n",
    "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
    "\n",
    "Assuming $G^\\ell$ is the Gram matrix from the feature map of the current image, $A^\\ell$ is the Gram Matrix from the feature map of the source style image, and $w_\\ell$ a scalar weight term, then the style loss for the layer $\\ell$ is simply the weighted Euclidean distance between the two Gram matrices:\n",
    "\n",
    "$$L_s^\\ell = w_\\ell \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "In practice we usually compute the style loss at a set of layers $\\mathcal{L}$ rather than just a single layer $\\ell$; then the total style loss is the sum of style losses at each layer:\n",
    "\n",
    "$$L_s = \\sum_{\\ell \\in \\mathcal{L}} L_s^\\ell$$\n",
    "\n",
    "We have implemented the Gram matrix computation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(features, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix from features.\n",
    "    \n",
    "    Inputs:\n",
    "    - features: PyTorch Variable of shape (N, C, H, W) giving features for\n",
    "      a batch of N images.\n",
    "    - normalize: optional, whether to normalize the Gram matrix\n",
    "        If True, divide the Gram matrix by the number of neurons (H * W * C)\n",
    "    \n",
    "    Returns:\n",
    "    - gram: PyTorch Variable of shape (N, C, C) giving the\n",
    "      (optionally normalized) Gram matrices for the N input images.\n",
    "    \"\"\"\n",
    "    a, b, c, d = features.size()  \n",
    "    features = features.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    if normalize:\n",
    "        return G.div(a * b * c * d)\n",
    "    return G\n",
    "                \n",
    "#     trans=features.transpose(1,2)\n",
    "#     print(trans.shape)\n",
    "#     gram=trans.dot(features)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the style loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now put it together in the style_loss function...\n",
    "def style_loss(feats, style_layers, style_targets, style_weights):\n",
    "    \"\"\"\n",
    "    Computes the style loss at a set of layers.\n",
    "    \n",
    "    Inputs:\n",
    "    - feats: list of the features at every layer of the current image, as produced by\n",
    "      the extract_features function.\n",
    "    - style_layers: List of layer indices into feats giving the layers to include in the\n",
    "      style loss.\n",
    "    - style_targets: List of the same length as style_layers, where style_targets[i] is\n",
    "      a PyTorch Variable giving the Gram matrix the source style image computed at\n",
    "      layer style_layers[i].\n",
    "    - style_weights: List of the same length as style_layers, where style_weights[i]\n",
    "      is a scalar giving the weight for the style loss at layer style_layers[i].\n",
    "      \n",
    "    Returns:\n",
    "    - style_loss: A PyTorch Variable holding a scalar giving the style loss.\n",
    "    \"\"\"\n",
    "#     for l in range(len(style_layers)):\n",
    "#         G=gram_matrix(feats[style_layers[l]])\n",
    "\n",
    "#         C,C2=G.size()\n",
    "#         summy=0\n",
    "        \n",
    "#         for i in range(C):\n",
    "#             for j in range(C2):\n",
    "#                 #print((i,j))\n",
    "#                 summy+=(G[i][j]-style_targets[l][i][j])**2\n",
    "#         losses.append(style_weights[l]*summy)\n",
    "#     return sum(losses)\n",
    "    losses=[]\n",
    "    for l in range(len(style_layers)):\n",
    "        G=gram_matrix(feats[style_layers[l]])\n",
    "        C,_=G.shape\n",
    "        #print(G,style_targets[l],)\n",
    "        #print(G.shape,style_targets[l].shape)\n",
    "        losses.append(style_weights[l]*(C**2*torch.nn.functional.mse_loss(G, style_targets[l])))\n",
    "    #print(losses)\n",
    "    return sum(losses)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total-variation regularization\n",
    "It turns out that it's helpful to also encourage smoothness in the image. We can do this by adding another term to our loss that penalizes wiggles or \"total variation\" in the pixel values. \n",
    "\n",
    "You can compute the \"total variation\" as the sum of the squares of differences in the pixel values for all pairs of pixels that are next to each other (horizontally or vertically). Here we sum the total-variation regualarization for each of the 3 input channels (RGB), and weight the total summed loss by the total variation weight, $w_t$:\n",
    "\n",
    "$L_{tv} = w_t \\times \\sum_{c=1}^3\\sum_{i=1}^{H-1} \\sum_{j=1}^{W-1} \\left( (x_{i,j+1, c} - x_{i,j,c})^2 + (x_{i+1, j,c} - x_{i,j,c})^2  \\right)$\n",
    "\n",
    "In the next cell, we fill in the definition for the TV loss term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss(img, tv_weight):\n",
    "    \"\"\"\n",
    "    Compute total variation loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - img: PyTorch Variable of shape (1, 3, H, W) holding an input image.\n",
    "    - tv_weight: Scalar giving the weight w_t to use for the TV loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Variable holding a scalar giving the total variation loss\n",
    "      for img weighted by tv_weight.\n",
    "    \"\"\"\n",
    "    # Your implementation should be vectorized and not require any loops!\n",
    "    diff_i = torch.sum(torch.square((img[:, :, :, 1:] - img[:, :, :, :-1])))\n",
    "    diff_j = torch.sum(torch.square(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :])))\n",
    "    tv_loss = tv_weight*(diff_i + diff_j)\n",
    "    return tv_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to string it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image, image_size, style_size, content_layer, content_weight,\n",
    "                   style_layers, style_weights, tv_weight, init_random = False, quiet=False, batchsize=1, tensors=False):\n",
    "    \"\"\"\n",
    "    Run style transfer!\n",
    "    \n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_image: filename of style image\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "    - style_size: size of smallest style image dimension\n",
    "    - content_layer: layer to use for content loss\n",
    "    - content_weight: weighting on content loss\n",
    "    - style_layers: list of layers to use for style loss\n",
    "    - style_weights: list of weights to use for each layer in style_layers\n",
    "    - tv_weight: weight of total variation regularization term\n",
    "    - init_random: initialize the starting image to uniform random noise\n",
    "    \"\"\"\n",
    "    if not tensors:\n",
    "        if batchsize!=1:\n",
    "        # Extract features for the content image\n",
    "    #         style_img = torch.cat([preprocess(PIL.Image.open(img), size=image_size) for img in style_image]).reshape(batchsize,3,image_size,image_size)        \n",
    "            content_img = torch.cat([preprocess(PIL.Image.open(img), size=image_size) for img in content_image]).reshape(batchsize,3,image_size,image_size)\n",
    "        else:\n",
    "            content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n",
    "    else:\n",
    "        content_img = preprocess_batch(content_image.to(device))\n",
    "    style_img = preprocess(PIL.Image.open(style_image), size=style_size).repeat(batchsize,1,1,1).to(device)\n",
    "\n",
    "    content_img_var = Variable(content_img.type(dtype)).to(device)\n",
    "    feats = extract_features(content_img_var, cnn)\n",
    "    content_target = feats[content_layer].clone().to(device)\n",
    "\n",
    "    # Extract features for the style image\n",
    "\n",
    "    style_img_var = Variable(style_img.type(dtype)).to(device)\n",
    "    feats = extract_features(style_img_var, cnn)\n",
    "    style_targets = []\n",
    "    for idx in style_layers:\n",
    "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
    "\n",
    "    # Initialize output image to content image or nois\n",
    "    if init_random:\n",
    "        img = torch.Tensor(content_img.size()).uniform_(0, 1)\n",
    "    else:\n",
    "        img = content_img.clone().type(dtype).to(device)\n",
    "    # We do want the gradient computed on our image!\n",
    "    img_var = Variable(img, requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 3.0\n",
    "    decayed_lr = 0.1\n",
    "    decay_lr_at = 180\n",
    "\n",
    "    # Note that we are optimizing the pixel values of the image by passing\n",
    "    # in the img_var Torch variable, whose requires_grad flag is set to True\n",
    "    optimizer = torch.optim.Adam([img_var], lr=initial_lr)\n",
    "    if not quiet:\n",
    "        f, axarr = plt.subplots(1,2)\n",
    "        axarr[0].axis('off')\n",
    "        axarr[1].axis('off')\n",
    "        axarr[0].set_title('Content Source Img.')\n",
    "        axarr[1].set_title('Style Source Img.')\n",
    "        axarr[0].imshow(deprocess(content_img.cpu()))\n",
    "        axarr[1].imshow(deprocess(style_img.cpu()))\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "\n",
    "    for t in range(200):\n",
    "        if t < 190:\n",
    "            img.clamp_(-1.5, 1.5)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        feats = extract_features(img_var, cnn)\n",
    "        #print(t)\n",
    "        # Compute loss\n",
    "        c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
    "        #print(t)\n",
    "        s_loss = style_loss(feats, style_layers, style_targets, style_weights)\n",
    "        #print(t)\n",
    "        t_loss = tv_loss(img_var, tv_weight)\n",
    "        #print(t)\n",
    "        loss = c_loss + s_loss + t_loss\n",
    "\n",
    "        loss.backward()\n",
    "        #print(t)\n",
    "        # Perform gradient descents on our image values\n",
    "        if t == decay_lr_at:\n",
    "            optimizer = torch.optim.Adam([img_var], lr=decayed_lr)\n",
    "        optimizer.step()\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            if not quiet:\n",
    "                print('Iteration {}'.format(t))\n",
    "                plt.axis('off')\n",
    "                plt.imshow(deprocess(img.cpu()))\n",
    "                plt.show()\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some pretty pictures!\n",
    "\n",
    "* The `content_image` is the filename of content image.\n",
    "* The `style_image` is the filename of style image.\n",
    "* The `image_size` is the size of smallest image dimension of the content image (used for content loss and generated image).\n",
    "* The `style_size` is the size of smallest style image dimension.\n",
    "* The `content_layer` specifies which layer to use for content loss.\n",
    "* The `content_weight` gives weighting on content loss in the overall loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).\n",
    "* `style_layers` specifies a list of which layers to use for style loss. \n",
    "* `style_weights` specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.\n",
    "* `tv_weight` specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styletrans(im1,im2, show=False, batchsize=1,tensors=False):\n",
    "    content_size = 224\n",
    "    style_size = 224\n",
    "    params1 = {\n",
    "        'content_image' : im1,\n",
    "        'style_image' : im2,\n",
    "        'image_size' : content_size,\n",
    "        'style_size' : style_size,\n",
    "        'content_layer' : 3,\n",
    "        'content_weight' : 1e-1,\n",
    "        'style_layers' : [1, 4, 6, 7],\n",
    "        'style_weights' : [300000*max(1,int(batchsize)*2), 1000*max(1,int(batchsize)*2), 15*max(1,int(batchsize)*2), 3*max(1,int(batchsize)*2)],\n",
    "        'tv_weight' : 5e-2,\n",
    "        'quiet': True,\n",
    "        \"batchsize\":batchsize,\n",
    "        \"tensors\":tensors\n",
    "    }\n",
    "    if tensors:\n",
    "        return style_transfer(**params1)\n",
    "\n",
    "    if batchsize == 1:\n",
    "        img = style_transfer(**params1)\n",
    "        arr = deprocess(img.cpu())\n",
    "        if show:\n",
    "            plt.axis('off')\n",
    "            plt.imshow(arr)\n",
    "            plt.show()\n",
    "        return cv2.cvtColor(np.asarray(arr), cv2.COLOR_RGB2BGR)\n",
    "    else:\n",
    "        outs=style_transfer(**params1).cpu()\n",
    "#         print(outs.shape)\n",
    "        imgs = deprocess(outs)\n",
    "#         print(imgs)\n",
    "        arrs = []\n",
    "        for arr in imgs:\n",
    "            arrs.append(cv2.cvtColor(np.asarray(arr), cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "            if show:\n",
    "                plt.axis('off')\n",
    "                plt.imshow(arr)\n",
    "                plt.show()\n",
    "        return arrs\n",
    "\n",
    "\n",
    "#720p Movie Maker\n",
    "def styletrans_a_mp4(movie, style, outputmovie,s=-1, batchsize=1):\n",
    "    cam = cv2.VideoCapture(movie)\n",
    "    frame_ct= int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
    "    fps = int(cam.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"FPS:\",fps)\n",
    "    video_filename = outputmovie\n",
    "    width = 224\n",
    "    height = 224\n",
    "    out = cv2.VideoWriter(video_filename, fourcc, fps, (width, height))\n",
    "    done_ct = 0\n",
    "    print(str(done_ct)+\"/\"+str(int((frame_ct if s==-1 else s*fps))))\n",
    "    if batchsize==1:\n",
    "        while(True):      \n",
    "            # reading from frame\n",
    "            ret,frame = cam.read()\n",
    "            if ret:\n",
    "                if s!=-1 and done_ct > int(s*fps):\n",
    "                    print(\"Stopping after \" + str(s) + \" seconds of footage\")\n",
    "                    out.release()\n",
    "                    return\n",
    "                done_ct+=1\n",
    "                name = 'xyzfdsf.jpg'\n",
    "                cv2.imwrite(name, frame)\n",
    "                if done_ct % int(fps/8) == 0:\n",
    "                    print(str(done_ct)+\"/\"+str(int((frame_ct if s==-1 else s*fps))))\n",
    "                img = styletrans(name, style, show = (done_ct ==1))\n",
    "                out.write(img)\n",
    "                os.remove(name)\n",
    "            else:\n",
    "                out.release()\n",
    "                return\n",
    "    else:\n",
    "        batch_count=0\n",
    "        names = []\n",
    "        batch_time_sum = 0\n",
    "        since= time.time()\n",
    "        numbatches = 0\n",
    "        avg_fps=-1\n",
    "        p = None\n",
    "        while(True):      \n",
    "            # reading from frame\n",
    "            ret,frame = cam.read()\n",
    "            \n",
    "            \n",
    "            if ret:\n",
    "                if s!=-1 and done_ct > int(s*fps):\n",
    "                    print(\"Stopping after \" + str(s) + \" seconds of footage\")\n",
    "                    out.release()\n",
    "                    for name in names:\n",
    "                        os.remove(name)\n",
    "                    return avg_fps\n",
    "                done_ct+=1\n",
    "                name = 'xyzfdsf' + str(numbatches) + '_' +str(batch_count)+ '.jpg'\n",
    "                cv2.imwrite(name, frame)\n",
    "                if done_ct % int(fps/8) == 0:\n",
    "                    print(str(done_ct)+\"/\"+str(int((frame_ct if s==-1 else s*fps))) + '  |  Average FPS:' +str(avg_fps))\n",
    "                if batch_count==batchsize:\n",
    "                    imgs = styletrans(names, style, show = (done_ct ==1), batchsize=batchsize)\n",
    "                    for img, name in zip(imgs, names):\n",
    "                        out.write(img)\n",
    "                        os.remove(name)\n",
    "                    batch_count = 0\n",
    "                    batch_time_sum+=time.time()-since\n",
    "                    names=[]\n",
    "                    since=time.time()\n",
    "                    numbatches+=1\n",
    "                    avg_fps = round(batch_time_sum/(batchsize*numbatches),2)\n",
    "                else:\n",
    "                    batch_count+=1\n",
    "                    names.append(name)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                out.release()\n",
    "                for name in names:\n",
    "                    os.remove(name)\n",
    "                return avg_fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test on the basics--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# arr = styletrans('styles/tubingen.jpg','styles/starry_night.jpg', show=True)\n",
    "\n",
    "# arrs = styletrans(['styles/tubingen.jpg','styles/tubingen.jpg','styles/tubingen.jpg'],'styles/starry_night.jpg', show=True, batchsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test movie style transfer\n",
    "# styletrans_a_mp4('styles/dog.mp4','styles/the_scream.jpg','styles/scream_dog2.mp4',batchsize=46,s=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_for_gpu():\n",
    "    best_fps=0\n",
    "    best_size=0\n",
    "    for size in range(2,1024,5):\n",
    "        try:\n",
    "            print(\"Profiling batch size:\",size)\n",
    "            fps = styletrans_a_mp4('styles/dog.mp4','styles/starry_night.jpg','styles/star_dog.mp4',batchsize=size,s=1)\n",
    "            if fps>best_fps:\n",
    "                best_fps=fps\n",
    "                best_size=size\n",
    "        except:\n",
    "            break\n",
    "    print(\"Fastest Batch Size for Current GPU:\",best_size)\n",
    "# tune_for_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to upload a video to youtube \n",
    "#requires client_secrets.json \n",
    "#according to https://developers.google.com/youtube/v3/guides/uploading_a_video\n",
    "import os\n",
    "def upload_yt(vidpth, title, show=False):\n",
    "    x= str('youtube-upload \"') + vidpth + '\" --title=' + '\"' + str(title) + '\"'\n",
    "    x+=\" --description=\" +'\"deepdream,art,deepstyle,style,AI,deeplearning,automation,neuralnet,trippy,hallucinate\"'\n",
    "    x+=\" --tags=\"+'\"deepdream,deepstyle,style,AI,art,deeplearning,automation,neuralnet,trippy,hallucinate\"'\n",
    "    x+=\" --privacy=\"+'\"public\"'\n",
    "    if show:\n",
    "        x+= \" --open-link\"\n",
    "    print(x)\n",
    "    !{x}\n",
    "from uploader import Uploader\n",
    "import json\n",
    "\n",
    "def selen_upload_yt(videopath,title):\n",
    "    file_path='dummy.json'\n",
    "    with open(file_path,'w+') as f:\n",
    "        x={\"title\":title,\"video_path\":videopath}\n",
    "#         print(x)\n",
    "        json.dump(x, f)\n",
    "#     print(open(file_path,\"r\").read())\n",
    "    uploader = Uploader(visible=False)\n",
    "    uploader.upload_video(file_path)\n",
    "    \n",
    "#     print(open(file_path,\"r\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[normalize-space()=\"No, it's not made for kids\"]\"}\n  (Session info: chrome=91.0.4472.77)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0bd00b9b4d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test upload code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mselen_upload_yt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'styles/scream_dog.mp4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Scream Dog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-c3c44a359a6e>\u001b[0m in \u001b[0;36mselen_upload_yt\u001b[0;34m(videopath, title)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     print(open(file_path,\"r\").read())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0muploader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUploader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0muploader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     print(open(file_path,\"r\").read())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/style_trans/DeepNet-Visualization/uploader.py\u001b[0m in \u001b[0;36mupload_video\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# ## TODO: Add info about video upload here!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mnokidsid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ink\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//button[normalize-space()=\"No, it\\'s not made for kids\"]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# Next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[normalize-space()=\"No, it's not made for kids\"]\"}\n  (Session info: chrome=91.0.4472.77)\n"
     ]
    }
   ],
   "source": [
    "#Test upload code\n",
    "selen_upload_yt(os.getcwd()+os.sep+'styles/scream_dog.mp4',\"Scream Dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Composition VII + Tubingen\n",
    "# params1 = {\n",
    "#     'content_image' : 'styles/tubingen.jpg',\n",
    "#     'style_image' : 'styles/composition_vii.jpg',\n",
    "#     'image_size' : 192,\n",
    "#     'style_size' : 512,\n",
    "#     'content_layer' : 3,\n",
    "#     'content_weight' : 5e-2, \n",
    "#     'style_layers' : (1, 4, 6, 7),\n",
    "#     'style_weights' : (20000, 500, 12, 1),\n",
    "#     'tv_weight' : 5e-2\n",
    "# }\n",
    "\n",
    "# style_transfer(**params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scream + Tubingen\n",
    "# params2 = {\n",
    "#     'content_image':'styles/tubingen.jpg',\n",
    "#     'style_image':'styles/the_scream.jpg',\n",
    "#     'image_size':192,\n",
    "#     'style_size':224,\n",
    "#     'content_layer':3,\n",
    "#     'content_weight':3e-2,\n",
    "#     'style_layers':[1, 4, 6, 7],\n",
    "#     'style_weights':[200000, 800, 12, 1],\n",
    "#     'tv_weight':2e-2\n",
    "# }\n",
    "\n",
    "# style_transfer(**params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Starry Night + Tubingen\n",
    "# params3 = {\n",
    "#     'content_image' : 'styles/tubingen.jpg',\n",
    "#     'style_image' : 'styles/starry_night.jpg',\n",
    "#     'image_size' : 192,\n",
    "#     'style_size' : 192,\n",
    "#     'content_layer' : 3,\n",
    "#     'content_weight' : 6e-2,\n",
    "#     'style_layers' : [1, 4, 6, 7],\n",
    "#     'style_weights' : [300000, 1000, 15, 3],\n",
    "#     'tv_weight' : 2e-2\n",
    "# }\n",
    "\n",
    "# style_transfer(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Feature Inversion\n",
    "\n",
    "The code written can do another cool thing. In an attempt to understand the types of features that convolutional networks learn to recognize, a recent paper [1] attempts to reconstruct an image from its feature representation. We can easily implement this idea using image gradients from the pretrained network, which is exactly what we did above (but with two different feature representations).\n",
    "\n",
    "Now, if you set the style weights to all be 0 and initialize the starting image to random noise instead of the content source image, you'll reconstruct an image from the feature representation of the content source image. You're starting with total noise, but you should end up with something that looks quite a bit like your original image.\n",
    "\n",
    "(Similarly, you could do \"texture synthesis\" from scratch if you set the content weight to 0 and initialize the starting image to random noise)\n",
    "\n",
    "[1] Aravindh Mahendran, Andrea Vedaldi, \"Understanding Deep Image Representations by Inverting them\", CVPR 2015\n",
    "\n",
    "Example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Inversion -- Starry Night + Tubingen\n",
    "# params_inv = {\n",
    "#     'content_image' : 'styles/tubingen.jpg',\n",
    "#     'style_image' : 'styles/starry_night.jpg',\n",
    "#     'image_size' : 192,\n",
    "#     'style_size' : 192,\n",
    "#     'content_layer' : 3,\n",
    "#     'content_weight' : 6e-2,\n",
    "#     'style_layers' : [1, 4, 6, 7],\n",
    "#     'style_weights' : [0, 0, 0, 0], # we discard any contributions from style to the loss\n",
    "#     'tv_weight' : 2e-2,\n",
    "#     'init_random': True # we want to initialize our image to be random\n",
    "# }\n",
    "\n",
    "# style_transfer(**params_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
